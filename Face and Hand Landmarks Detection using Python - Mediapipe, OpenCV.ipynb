{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e71ff5-554f-4a63-8884-31cf5488ca59",
   "metadata": {},
   "source": [
    "In this article, we will use mediapipe python library to detect face and hand landmarks. We will be using a Holistic model from mediapipe solutions to detect all the face and hand landmarks. We will be also seeing how we can access different landmarks of the face and hands which can be used for different computer vision applications such as sign language detection, drowsiness detection, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6bd9f-d042-4704-ad53-8f73882b257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python mediapipe msvc-runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a88920-a308-4c87-aac9-77a86d47e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f02435-260c-4be7-b550-5f7f8408a558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the Holistic Model from Mediapipe and\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Initializing the drawing utils for drawing the facial landmarks on image\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c7aeb-d705-429a-9f23-9a1cff9b2f4e",
   "metadata": {},
   "source": [
    "Let us look into the parameters for the Holistic Model:\n",
    "\n",
    "# Holistic(\n",
    "  static_image_mode=False, \n",
    "  model_complexity=1, \n",
    "  smooth_landmarks=True, \n",
    "  min_detection_confidence=0.5, \n",
    "  min_tracking_confidence=0.5\n",
    ") \n",
    "--\n",
    "----\n",
    "* static_image_mode: It is used to specify whether the input images must be treated as static images or as a video stream. The default value is False.\n",
    "* model_complexity: It is used to specify the complexity of the pose landmark model: 0, 1, or 2. As the model complexity of the model increases the landmark accuracy and latency increase. The default value is 1.\n",
    "* smooth_landmarks: This parameter is used to reduce the jitter in the prediction by filtering pose landmarks across different input images. The default value is True.\n",
    "* min_detection_confidence: It is used to specify the minimum confidence value with which the detection from the person-detection model needs to be considered as successful. Can specify a value in [0.0,1.0]. The default value is 0.5.\n",
    "* min_tracking_confidence: It is used to specify the minimum confidence value with which the detection from the landmark-tracking model must be considered as successful. Can specify a value in [0.0,1.0]. The default value is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84a72a-192a-4bc4-810a-b22d98b00f42",
   "metadata": {},
   "source": [
    "# Without contour line on Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc8a1c70-f8d3-429d-888d-b75fd850cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing current time and precious time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "while capture.isOpened():\n",
    "    # capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # resizing the frame for better view\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Converting the from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Converting back the RGB image to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image,\n",
    "        results.face_landmarks,\n",
    "        connections=None,  # ðŸ‘ˆ no connections, just points\n",
    "        landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "            color=(0,255,0),   # green dots\n",
    "            thickness=1,\n",
    "            circle_radius=2\n",
    "        ),\n",
    "        connection_drawing_spec=None  # ðŸ‘ˆ disable drawing connections\n",
    "    )\n",
    "\n",
    "\n",
    "    # Drawing Right hand Land Marks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, \n",
    "      results.right_hand_landmarks, \n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    # Drawing Left hand Land Marks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, \n",
    "      results.left_hand_landmarks, \n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "    \n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime-previousTime)\n",
    "    previousTime = currentTime\n",
    "    \n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When all the process is done\n",
    "# Release the capture and destroy all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65b6333-5ca3-4e76-a97e-e758cde0cacc",
   "metadata": {},
   "source": [
    "# With Contour Lines on Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "512546e1-ad7d-47a9-8119-b53b3059fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0) in VideoCapture is used to connect to your computer's default camera\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing current time and precious time for calculating the FPS\n",
    "previousTime = 0\n",
    "currentTime = 0\n",
    "\n",
    "while capture.isOpened():\n",
    "    # capture frame by frame\n",
    "    ret, frame = capture.read()\n",
    "\n",
    "    # resizing the frame for better view\n",
    "    frame = cv2.resize(frame, (800, 600))\n",
    "\n",
    "    # Converting the from BGR to RGB\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Making predictions using holistic model\n",
    "    # To improve performance, optionally mark the image as not writeable to\n",
    "    # pass by reference.\n",
    "    image.flags.writeable = False\n",
    "    results = holistic_model.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    # Converting back the RGB image to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Drawing the Facial Landmarks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image,\n",
    "      results.face_landmarks,\n",
    "      mp_holistic.FACEMESH_CONTOURS,    #FACEMESH_CONTOURS draws the outline of the face (jawline, eyes, mouth, etc.)\n",
    "      mp_drawing.DrawingSpec(\n",
    "        color=(255,0,255),\n",
    "        thickness=1,\n",
    "        circle_radius=1\n",
    "      ),\n",
    "      mp_drawing.DrawingSpec(\n",
    "        color=(0,255,255),\n",
    "        thickness=1,\n",
    "        circle_radius=1\n",
    "      )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Drawing Right hand Land Marks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, \n",
    "      results.right_hand_landmarks, \n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "\n",
    "    # Drawing Left hand Land Marks\n",
    "    mp_drawing.draw_landmarks(\n",
    "      image, \n",
    "      results.left_hand_landmarks, \n",
    "      mp_holistic.HAND_CONNECTIONS\n",
    "    )\n",
    "    \n",
    "    # Calculating the FPS\n",
    "    currentTime = time.time()\n",
    "    fps = 1 / (currentTime-previousTime)\n",
    "    previousTime = currentTime\n",
    "    \n",
    "    # Displaying FPS on the image\n",
    "    cv2.putText(image, str(int(fps))+\" FPS\", (10, 70), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow(\"Facial and Hand Landmarks\", image)\n",
    "\n",
    "    # Enter key 'q' to break the loop\n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When all the process is done\n",
    "# Release the capture and destroy all windows\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4db5f4e-bc80-482d-871f-e799b7bf0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqqqqqqqq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
